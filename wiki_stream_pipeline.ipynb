{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f404ce-23db-40fa-956b-f9a52c4c06f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85660588-1cf6-485e-aab3-00abc38ec491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database():\n",
    "    conn = sqlite3.connect(\"live_wikipedia.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS wiki_edits (\n",
    "            timestamp TEXT,\n",
    "            user_name TEXT,\n",
    "            page_title TEXT,\n",
    "            is_bot BOOLEAN,\n",
    "            edit_length_change INTEGER,\n",
    "            wikipedia_domain TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e69498-4f13-4523-951a-12eace7401ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to Wikipedia's Live Data Firehose...\n",
      "üì° Connected! Listening for live global edits...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "HTTP Status Code: 200 (Should be 200)\n",
      "[17:46:31] üë§ HUMAN edited: 'Jerzy Duracz' (6 chars) on pl.wikipedia.org\n",
      "[17:46:31] üë§ HUMAN edited: 'Q4129770' (71 chars) on www.wikidata.org\n",
      "[17:46:31] üë§ HUMAN edited: 'Q137326281' (1071 chars) on www.wikidata.org\n",
      "[17:46:31] ü§ñ BOT edited: 'Q124201674' (-2 chars) on www.wikidata.org\n",
      "[17:46:31] üë§ HUMAN edited: 'Simone Consonni' (-16 chars) on de.wikipedia.org\n",
      "[17:46:31] üë§ HUMAN edited: 'Q4129770' (13 chars) on www.wikidata.org\n",
      "[17:46:32] ü§ñ BOT edited: 'Richard Thomas' (11 chars) on pt.wikipedia.org\n",
      "[17:46:32] üë§ HUMAN edited: 'Q4129770' (20 chars) on www.wikidata.org\n",
      "[17:46:32] üë§ HUMAN edited: 'Q137132465' (1071 chars) on www.wikidata.org\n",
      "[17:46:32] üë§ HUMAN edited: 'List fan Fryske bierbrouwerijen' (0 chars) on fy.wikipedia.org\n",
      "[17:46:32] üë§ HUMAN edited: 'Power network design (IC)' (33 chars) on en.wikipedia.org\n",
      "[17:46:32] üë§ HUMAN edited: 'korubaan' (1 chars) on ms.wiktionary.org\n",
      "[17:46:32] üë§ HUMAN edited: 'Inventaire national du patrimoine naturel' (2 chars) on fr.wikipedia.org\n",
      "[17:46:32] üë§ HUMAN edited: 'Q4129770' (-3 chars) on www.wikidata.org\n",
      "\n",
      "üõë Pipeline stopped by user. Closing database connection.\n"
     ]
    }
   ],
   "source": [
    "def stream_wikipedia_data():\n",
    "    print(\"üîå Connecting to Wikipedia's Live Data Firehose...\", flush=True)\n",
    "    \n",
    "    url = 'https://stream.wikimedia.org/v2/stream/recentchange'\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'DataEngineeringPoC/1.0 (Python/requests)'\n",
    "    }\n",
    "    \n",
    "    conn = setup_database()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"üì° Connected! Listening for live global edits...\\n\", flush=True)\n",
    "    print(\"-\" * 70, flush=True)\n",
    "    \n",
    "    try:\n",
    "        with requests.get(url, headers=headers, stream=True, timeout=None) as response:\n",
    "            \n",
    "            print(f\"HTTP Status Code: {response.status_code} (Should be 200)\", flush=True)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(\"‚ùå Connection refused by Wikipedia. Stopping.\")\n",
    "                return\n",
    "\n",
    "            for line in response.iter_lines(chunk_size=1):\n",
    "                if line:\n",
    "                    decoded_line = line.decode('utf-8')\n",
    "                    \n",
    "                    if decoded_line.startswith('data: '):\n",
    "                        try:\n",
    "                            json_data = json.loads(decoded_line[6:])\n",
    "                            \n",
    "                            if json_data.get('type') == 'edit' and json_data.get('namespace') == 0:\n",
    "                                \n",
    "                                user = json_data.get('user', 'Unknown')\n",
    "                                title = json_data.get('title', 'Unknown')\n",
    "                                is_bot = json_data.get('bot', False)\n",
    "                                domain = json_data.get('meta', {}).get('domain', 'Unknown')\n",
    "                                \n",
    "                                old_len = json_data.get('length', {}).get('old', 0)\n",
    "                                new_len = json_data.get('length', {}).get('new', 0)\n",
    "                                length_diff = new_len - old_len\n",
    "                                \n",
    "                                current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                                \n",
    "                                cursor.execute('''\n",
    "                                    INSERT INTO wiki_edits (timestamp, user_name, page_title, is_bot, edit_length_change, wikipedia_domain)\n",
    "                                    VALUES (?, ?, ?, ?, ?, ?)\n",
    "                                ''', (current_time, user, title, is_bot, length_diff, domain))\n",
    "                                conn.commit()\n",
    "                                \n",
    "                                bot_status = \"ü§ñ BOT\" if is_bot else \"üë§ HUMAN\"\n",
    "                                print(f\"[{current_time}] {bot_status} edited: '{title}' ({length_diff} chars) on {domain}\", flush=True)\n",
    "                                \n",
    "                        except json.JSONDecodeError:\n",
    "                            continue \n",
    "                            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüõë Pipeline stopped by user. Closing database connection.\", flush=True)\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Pipeline crashed: {e}\", flush=True)\n",
    "        conn.close()\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    stream_wikipedia_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017dccf6-238e-4013-908e-8983c3190cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing Wikipedia Stream Data...\n",
      "\n",
      "Total Edits Captured: 3870\n",
      "----------------------------------------\n",
      "Who is making the edits?\n",
      "User_Type  Edit_Count\n",
      " Humans üë§        3176\n",
      "   Bots ü§ñ         694\n",
      "----------------------------------------\n",
      "Top 5 Most Heavily Modified Pages:\n",
      "                               page_title  Total_Chars_Changed\n",
      "                                  America                74784\n",
      "                     ‡¶ï‡ßá‡¶≤‡¶æ‡¶∏‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶® ‡¶°‡ßá‡¶ü‡¶æ‡¶¨‡ßá‡¶∏                59878\n",
      "                                   ÿ≠ÿ∂ÿ±ŸÖŸàÿ™                53943\n",
      "                               Q138481572                34885\n",
      "2022 Punjab Legislative Assembly election                23227\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_live_data():\n",
    "    print(\"üìä Analyzing Wikipedia Stream Data...\\n\")\n",
    "    \n",
    "    conn = sqlite3.connect(\"live_wikipedia.db\")\n",
    "    \n",
    "    total_edits = pd.read_sql_query(\"SELECT COUNT(*) as Total_Edits FROM wiki_edits\", conn)\n",
    "    print(f\"Total Edits Captured: {total_edits['Total_Edits'][0]}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    bot_ratio = pd.read_sql_query('''\n",
    "        SELECT \n",
    "            CASE WHEN is_bot = 1 THEN 'Bots ü§ñ' ELSE 'Humans üë§' END as User_Type,\n",
    "            COUNT(*) as Edit_Count\n",
    "        FROM wiki_edits\n",
    "        GROUP BY is_bot\n",
    "    ''', conn)\n",
    "    print(\"Who is making the edits?\")\n",
    "    print(bot_ratio.to_string(index=False))\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "\n",
    "    top_pages = pd.read_sql_query('''\n",
    "        SELECT page_title, SUM(ABS(edit_length_change)) as Total_Chars_Changed\n",
    "        FROM wiki_edits\n",
    "        GROUP BY page_title\n",
    "        ORDER BY Total_Chars_Changed DESC\n",
    "        LIMIT 5\n",
    "    ''', conn)\n",
    "    \n",
    "    print(\"Top 5 Most Heavily Modified Pages:\")\n",
    "    print(top_pages.to_string(index=False))\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "# Run the analysis\n",
    "analyze_live_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccff9a-40d6-47df-b5fe-07643b7ea48f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
